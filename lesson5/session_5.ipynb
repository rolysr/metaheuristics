{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metaheuristic course - Session 4\n",
    "\n",
    "> For this session you will need to install the sklearn python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make a pipeline\n",
    "\n",
    "A pipeline is a concatenation of structures that can help us process a dataset and train a model for a given task. In this session we will be building simple pipelines using a few transformation structures and some clasification estimators.\n",
    "\n",
    "Use the `get_pipeline` function defined below to build a pipeline. The arguments of this function are:\n",
    "\n",
    "- `pca_pos`: (Integer between 0 and 2, or equals -1) Position of the PCA in the pipeline. If -1 then no PCA is used.\n",
    "- `pca_n_components`: If PCA is used, this defines the `n_components` PCA hyperparameter.\n",
    "- `normalizer_pos`: (Integer between 0 and 2, or equals -1) Position of the Normalizer in the pipeline. If -1 then no Normalizer is used.\n",
    "- `normalizer_norm`: If Normalizer is used, this defines the `norm` Normalizer hyperparameter.\n",
    "- `standar_scaler_pos`: (Integer between 0 and 2, or equals -1) Position of the StandarScaler in the pipeline. If -1 then no StandarScaler is used.\n",
    "- `use_rfc`: (Boolean) Defines if the RandomForestCalsifier is used. \n",
    "- `rfc_n_estimators`: If RFC is used, this defines the `n_estimators` hyperparameter.\n",
    "- `rfc_max_depth`: If RFC is used, this defines the `n_estimators` hyperparameter.\n",
    "- `use_knc`: (Boolean) Defines if the KNeighborsClasifier is used.\n",
    "- `knc_n_neighbors`: If KNC is used, this defines the `n_neighbors` hyperparameter.\n",
    "- `use_svc`: (Boolean) Defines if the RandomForestCalsifier is used.\n",
    "- `svc_c`: If SVC is used, this defines the `C` hyperparameter.\n",
    "- `svc_degree`: If SVC is used, this defines the `degree` hyperparameter.\n",
    "\n",
    "> The default values for the hyperparameters are the same as the ones defined by sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(\n",
    "    pca_pos: int = -1,\n",
    "    pca_n_components=None,\n",
    "    normalizer_pos: int = -1,\n",
    "    normalizer_norm: str = \"l2\",\n",
    "    standar_scaler_pos: int = -1,\n",
    "    use_rfc: bool = False,\n",
    "    rfc_n_estimators: int = 100,\n",
    "    rfc_max_depth: Union[int, None] = None,\n",
    "    use_knc: bool = False,\n",
    "    knc_n_neighbors: int = 5,\n",
    "    use_svc: bool = False,\n",
    "    svc_c: float = 1.0,\n",
    "    svc_degree: int = 3,\n",
    ") -> Pipeline:\n",
    "    pipeline: List[Union[None, Tuple]] = [None] * 4\n",
    "\n",
    "    assert pca_pos == -1 or 0 <= pca_pos <= 2\n",
    "    assert normalizer_pos == -1 or 0 <= normalizer_pos <= 2\n",
    "    assert standar_scaler_pos == -1 or 0 <= standar_scaler_pos <= 2\n",
    "\n",
    "    if pca_pos >= 0:\n",
    "        pipeline[pca_pos] = (\"pca\", PCA(n_components=pca_n_components))\n",
    "\n",
    "    if normalizer_pos >= 0:\n",
    "        pipeline[normalizer_pos] = (\"normalizer\", Normalizer(norm=normalizer_norm))\n",
    "\n",
    "    if standar_scaler_pos >= 0:\n",
    "        pipeline[standar_scaler_pos] = (\"scaler\", StandardScaler())\n",
    "\n",
    "    pipeline = [item for item in pipeline if item is not None]\n",
    "        \n",
    "    assert sum((use_knc, use_rfc, use_svc)) == 1, \"Exactly one classifier must be defined\"\n",
    "\n",
    "    if use_rfc:\n",
    "        pipeline.append(\n",
    "            (\n",
    "                \"rdf\",\n",
    "                RandomForestClassifier(\n",
    "                    n_estimators=rfc_n_estimators,\n",
    "                    max_depth=rfc_max_depth,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    if use_knc:\n",
    "        pipeline.append(\n",
    "            (\n",
    "                \"knc\",\n",
    "                KNeighborsClassifier(\n",
    "                    n_neighbors=knc_n_neighbors,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    if use_svc:\n",
    "        pipeline.append(\n",
    "            (\n",
    "                \"svc\",\n",
    "                SVC(\n",
    "                    C=svc_c,\n",
    "                    degree=svc_degree,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return Pipeline(pipeline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's define some pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('knc', KNeighborsClassifier())])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pipeline(use_knc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('normalizer', Normalizer()), ('pca', PCA()),\n",
       "                ('rdf', RandomForestClassifier(n_estimators=10))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pipeline(pca_pos=1, normalizer_pos=0, use_rfc=True, rfc_n_estimators=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have build your pipeline you can train and test the estimator against a dataset. For example, let's use the iris sklearn dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9622641509433962"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_ds = load_iris()\n",
    "X, y = iris_ds[\"data\"], iris_ds[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35)\n",
    "\n",
    "p2 = get_pipeline(pca_pos=1, normalizer_pos=0, use_rfc=True, rfc_n_estimators=10)\n",
    "p2.fit(X_train, y_train)\n",
    "p2.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this secuence of steps (pipeline) can predict very well the data from the iris dataset. However this is a small and very simple dataset. You can find more information about the sklearn dataset [here](https://scikit-learn.org/stable/datasets/toy_dataset.html).\n",
    "\n",
    "So, the question is: given a dataset, what is the best pipeline you can build for a given task (e.g. clasification)?\n",
    "\n",
    "Your task today is to build an heuristic that finds that pipeline using the tools of this notebook :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Pipeline(steps=[('scaler', StandardScaler()), ('knc', KNeighborsClassifier())]), 1.0)\n"
     ]
    }
   ],
   "source": [
    "from random import uniform\n",
    "\n",
    "class Estimator:\n",
    "    def __init__(self) -> None:\n",
    "        self.p_pca_pos = {-1:1/4, 0:1/4, 1:1/4, 2:1/4}\n",
    "        self.p_normalizer_pos = {-1:1/4, 0:1/4, 1:1/4, 2:1/4}\n",
    "        self.p_standard_scaler_pos = {-1:1/4, 0:1/4, 1:1/4, 2:1/4}\n",
    "        self.p_model = {1: 1/3, 2:1/3, 3:1/3}\n",
    "    \n",
    "    def sample_individual(self):\n",
    "        u1 = uniform(0, 1)\n",
    "        u2 = uniform(0, 1)\n",
    "        u3 = uniform(0, 1)\n",
    "        u4 = uniform(0, 1)\n",
    "\n",
    "        dist_pca_pos = [0 for i in range(4)]\n",
    "        dist_normalizer_pos = [0 for i in range(4)]\n",
    "        dist_stardard_scaler_pos = [0 for i in range(4)]\n",
    "        dist_model = [0 for i in range(3)]\n",
    "\n",
    "        pca_keys = list(self.p_pca_pos.keys())\n",
    "        normalizer_keys = list(self.p_normalizer_pos.keys())\n",
    "        standard_scaler_keys = list(self.p_standard_scaler_pos.keys())\n",
    "        model_keys = list(self.p_model.keys())\n",
    "        for i in range(4):\n",
    "            dist_pca_pos[i] = self.p_pca_pos[pca_keys[i]] + (0 if i == 0 else dist_pca_pos[i-1])\n",
    "            dist_normalizer_pos[i] = self.p_normalizer_pos[normalizer_keys[i]] + (0 if i == 0 else dist_normalizer_pos[i-1])\n",
    "            dist_stardard_scaler_pos[i] = self.p_standard_scaler_pos[standard_scaler_keys[i]] + (0 if i == 0 else dist_stardard_scaler_pos[i-1])\n",
    "\n",
    "        for i in range(3):\n",
    "            dist_model[i] = self.p_model[model_keys[i]] + (0 if i == 0 else dist_model[i-1])\n",
    "\n",
    "        pca_pos = pca_keys[self.estimate_discrete_uniform(dist_pca_pos, u1)]\n",
    "        normalizer_pos = normalizer_keys[self.estimate_discrete_uniform(dist_normalizer_pos, u2)]\n",
    "        standard_scaler_pos = standard_scaler_keys[self.estimate_discrete_uniform(dist_stardard_scaler_pos, u3)]\n",
    "        model = model_keys[self.estimate_discrete_uniform(dist_model, u4)]\n",
    "        return [pca_pos, normalizer_pos, standard_scaler_pos, model]\n",
    "\n",
    "    def estimate_discrete_uniform(self, distribution, u):\n",
    "        assert 0 <= u <= 1\n",
    "        index = 0\n",
    "        for value in distribution:\n",
    "            if u < value:\n",
    "                return index\n",
    "            index += 1\n",
    "        return index\n",
    "\n",
    "def goal_function_iris(pipeline):\n",
    "    iris_ds = load_iris()\n",
    "    X, y = iris_ds[\"data\"], iris_ds[\"target\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    return p2.score(X_test, y_test)\n",
    "\n",
    "def umda_classification(max_generations=20, population_size=20):\n",
    "    estimator = Estimator()\n",
    "    population = generate_population(estimator, population_size)\n",
    "    best_pipeline = None\n",
    "    best_value = 0\n",
    "    for i in range(max_generations):\n",
    "        best_individuals = get_best_individuals(population, population_size/2)\n",
    "        fine_tune_estimator(estimator, best_individuals)\n",
    "        population = generate_population(estimator, population_size)\n",
    "        best_pipeline, best_value = get_best_pipeline(population, best_pipeline, best_value)\n",
    "    return best_pipeline, best_value\n",
    "\n",
    "def get_best_pipeline(population, best_pipeline, best_value):\n",
    "    current_best_pipeline = best_pipeline\n",
    "    current_best_value = best_value\n",
    "    for individual in population:\n",
    "        pipeline = get_pipeline_by_individual(individual)\n",
    "        evaluation = goal_function_iris(pipeline)\n",
    "        if evaluation > current_best_value:\n",
    "            current_best_pipeline = pipeline\n",
    "            current_best_value = evaluation\n",
    "    return current_best_pipeline, current_best_value\n",
    "\n",
    "def fine_tune_estimator(estimator: Estimator, best_individuals):\n",
    "    preprocessors_count = [{i:0 for i in range(-1, 3)} for i in range(3)]\n",
    "    size = len(best_individuals)\n",
    "    models_count = [0 for i in range(3)]\n",
    "    for ind in best_individuals:\n",
    "        pca_pos, normalizer_pos, standard_scaler_pos, model = ind\n",
    "        preprocessors_count[0][pca_pos] += 1\n",
    "        preprocessors_count[1][normalizer_pos] += 1\n",
    "        preprocessors_count[2][standard_scaler_pos] += 1\n",
    "        models_count[model-1]+=1\n",
    "    \n",
    "    for i in range(-1, 3):\n",
    "        estimator.p_pca_pos[i] = preprocessors_count[0][i]/size\n",
    "        estimator.p_normalizer_pos[i] = preprocessors_count[1][i]/size\n",
    "        estimator.p_standard_scaler_pos[i] = preprocessors_count[2][i]/size\n",
    "    \n",
    "    for i in range(1, 4):\n",
    "        estimator.p_model[i] = models_count[i-1]/size\n",
    "\n",
    "def get_best_individuals(population: List[List], size):\n",
    "    for i in range(len(population)):\n",
    "        individual = population[i]\n",
    "        pipeline = get_pipeline_by_individual(individual)\n",
    "        fitness = goal_function_iris(pipeline)\n",
    "        individual.insert(0, fitness)\n",
    "    population.sort()\n",
    "    result_population = []\n",
    "    count = 0\n",
    "    for ind in population:\n",
    "        if count == size:\n",
    "            break\n",
    "        result_population.append([ind[1], ind[2], ind[3], ind[4]])\n",
    "        count += 1\n",
    "    return result_population\n",
    "\n",
    "def generate_population(estimator, size=20):\n",
    "    population = []\n",
    "    for i in range(size):\n",
    "        population.append(estimator.sample_individual())\n",
    "    return population\n",
    "\n",
    "def get_pipeline_by_individual(individual: list):\n",
    "    pca_pos, normalizer_pos, standard_scaler_pos, model = individual\n",
    "    pipeline = None\n",
    "    if model == 1:\n",
    "        pipeline = get_pipeline(pca_pos=pca_pos, normalizer_pos=normalizer_pos, standar_scaler_pos=standard_scaler_pos, use_knc=True)\n",
    "    elif model == 2:\n",
    "        pipeline = get_pipeline(pca_pos=pca_pos, normalizer_pos=normalizer_pos, standar_scaler_pos=standard_scaler_pos, use_rfc=True)\n",
    "    elif model == 3:\n",
    "        pipeline = get_pipeline(pca_pos=pca_pos, normalizer_pos=normalizer_pos, standar_scaler_pos=standard_scaler_pos, use_svc=True)\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "print(umda_classification())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
